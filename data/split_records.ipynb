{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This file move all tfrecord files randomly based on their location to sub folders of train test and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_files_in_directory(directory):\n",
    "    \"\"\"\n",
    "    Unzips all zip files in the given directory.\n",
    "\n",
    "    Args:\n",
    "    - directory (str): The path to the directory containing zip files.\n",
    "    \"\"\"\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith('.zip'):\n",
    "            zip_path = os.path.join(directory, file)\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                print(f\"Unzipping {file}...\")\n",
    "                zip_ref.extractall(directory)\n",
    "                print(f\"Unzipped {file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping asset-index-2-20240103T071400Z-001.zip...\n",
      "Unzipped asset-index-2-20240103T071400Z-001.zip.\n",
      "Unzipping asset-index-2-20240103T071400Z-002.zip...\n",
      "Unzipped asset-index-2-20240103T071400Z-002.zip.\n",
      "Unzipping asset-index-2-20240103T071400Z-003.zip...\n",
      "Unzipped asset-index-2-20240103T071400Z-003.zip.\n",
      "Unzipping asset-index-2-20240103T071400Z-004.zip...\n",
      "Unzipped asset-index-2-20240103T071400Z-004.zip.\n",
      "Unzipping asset-index-2-20240103T071400Z-005.zip...\n",
      "Unzipped asset-index-2-20240103T071400Z-005.zip.\n",
      "Unzipping asset-index-20231229T004625Z-001.zip...\n",
      "Unzipped asset-index-20231229T004625Z-001.zip.\n",
      "Unzipping asset-index-20231229T004625Z-002.zip...\n",
      "Unzipped asset-index-20231229T004625Z-002.zip.\n",
      "Unzipping asset-index-20231229T004625Z-003.zip...\n",
      "Unzipped asset-index-20231229T004625Z-003.zip.\n",
      "Unzipping asset-index-20231229T004625Z-004.zip...\n",
      "Unzipped asset-index-20231229T004625Z-004.zip.\n",
      "Unzipping asset-index-20231229T004625Z-005.zip...\n",
      "Unzipped asset-index-20231229T004625Z-005.zip.\n",
      "Unzipping asset-index-20231229T004625Z-006.zip...\n",
      "Unzipped asset-index-20231229T004625Z-006.zip.\n",
      "Unzipping asset-index-20231229T004625Z-007.zip...\n",
      "Unzipped asset-index-20231229T004625Z-007.zip.\n",
      "Unzipping asset-index-20231229T004625Z-008.zip...\n",
      "Unzipped asset-index-20231229T004625Z-008.zip.\n"
     ]
    }
   ],
   "source": [
    "unzip_files_in_directory('raw_records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import glob\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "def extract_location_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extracts the longitude and latitude from the filename.\n",
    "\n",
    "    Args:\n",
    "    - filename (str): The filename to parse.\n",
    "\n",
    "    Returns:\n",
    "    - (float, float): A tuple of longitude and latitude.\n",
    "    \"\"\"\n",
    "    # Assuming filename format: \"year_longitude_latitude_...\"\n",
    "    parts = filename.split('_')\n",
    "    longitude = float(parts[1])\n",
    "    latitude = float(parts[2])\n",
    "    return (longitude, latitude)\n",
    "\n",
    "def split_files_by_location(directory, train_ratio=0.6, validation_ratio=0.2, test_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Splits files into training, validation, and test sets based on their location.\n",
    "\n",
    "    Args:\n",
    "    - directory (str): Directory containing the files.\n",
    "    - train_ratio (float): Ratio of training set size to total.\n",
    "    - validation_ratio (float): Ratio of validation set size to total.\n",
    "    - test_ratio (float): Ratio of test set size to total.\n",
    "\n",
    "    Returns:\n",
    "    - train_files, val_files, test_files (tuple): Lists of file paths for training, validation, and test.\n",
    "    \"\"\"\n",
    "    assert train_ratio + validation_ratio + test_ratio == 1, \"Ratios must sum up to 1\"\n",
    "\n",
    "    file_paths = glob.glob(os.path.join(directory, '*.tfrecord.gz'))\n",
    "    file_paths.sort()\n",
    "\n",
    "    # Group files by location\n",
    "    location_groups = {}\n",
    "    for path in file_paths:\n",
    "        filename = os.path.basename(path)\n",
    "        location = extract_location_from_filename(filename)\n",
    "        if location not in location_groups:\n",
    "            location_groups[location] = []\n",
    "        location_groups[location].append(path)\n",
    "\n",
    "    # Split locations into train, validation, and test sets\n",
    "    locations = list(location_groups.keys())\n",
    "    random.shuffle(locations)\n",
    "\n",
    "    train_end = int(len(locations) * train_ratio)\n",
    "    val_end = train_end + int(len(locations) * validation_ratio)\n",
    "\n",
    "    train_locations = set(locations[:train_end])\n",
    "    val_locations = set(locations[train_end:val_end])\n",
    "    test_locations = set(locations[val_end:])\n",
    "\n",
    "    # Allocate files to train, validation, or test sets\n",
    "    train_files, val_files, test_files = [], [], []\n",
    "    for location, paths in location_groups.items():\n",
    "        if location in train_locations:\n",
    "            train_files.extend(paths)\n",
    "        elif location in val_locations:\n",
    "            val_files.extend(paths)\n",
    "        elif location in test_locations:\n",
    "            test_files.extend(paths)\n",
    "\n",
    "    return train_files, val_files, test_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = split_files_by_location('raw_records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def move_files_to_subfolders(train_files, val_files, test_files, base_directory):\n",
    "    \"\"\"\n",
    "    Moves training and validation files into respective subfolders.\n",
    "\n",
    "    Args:\n",
    "    - train_files (list): List of training file paths.\n",
    "    - val_files (list): List of validation file paths.\n",
    "    - test_files (list): List of test file paths\n",
    "    - base_directory (str): The base directory where the subfolders will be created.\n",
    "    \"\"\"\n",
    "\n",
    "    train_dir = os.path.join(base_directory, 'train')\n",
    "    val_dir = os.path.join(base_directory, 'val')\n",
    "    test_dir = os.path.join(base_directory, 'test')\n",
    "\n",
    "    # Create subfolders if they don't exist\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(val_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    # Move training files\n",
    "    for file in train_files:\n",
    "        shutil.move(file, train_dir)\n",
    "    print(f\"Moved {len(train_files)} files to {train_dir}\")\n",
    "\n",
    "    # Move validation files\n",
    "    for file in val_files:\n",
    "        shutil.move(file, val_dir)\n",
    "    print(f\"Moved {len(val_files)} files to {val_dir}\")\n",
    "\n",
    "    # Move validation files\n",
    "    for file in test_files:\n",
    "        shutil.move(file, test_dir)\n",
    "    print(f\"Moved {len(test_files)} files to {test_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved 7867 files to raw_records\\train\n",
      "Moved 2636 files to raw_records\\val\n",
      "Moved 2637 files to raw_records\\test\n"
     ]
    }
   ],
   "source": [
    "move_files_to_subfolders(train, val, test, 'raw_records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the files and concatenate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import torch\n",
    "from dataloader import read_tfrecord\n",
    "from tqdm import tqdm\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_paths_shuffled(directory, file_extension='.tfrecord.gz'):\n",
    "    \"\"\"\n",
    "    Get all file paths in the specified directory with the specified file extension.\n",
    "\n",
    "    Args:\n",
    "    - directory (str): The directory to search for files.\n",
    "    - file_extension (str): The file extension to filter by.\n",
    "\n",
    "    Returns:\n",
    "    - List[str]: A list of file paths.\n",
    "    \"\"\"\n",
    "    file_paths = []\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(file_extension):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            file_paths.append(file_path)\n",
    "    random.seed(42)\n",
    "    random.shuffle(file_paths)\n",
    "    return file_paths\n",
    "\n",
    "def concatenate_and_save_tfrecords(tfrecord_paths, type):\n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "    unread_paths = []\n",
    "    count = 0\n",
    "    batch_number = 1\n",
    "    for path in tqdm(tfrecord_paths):\n",
    "        try:\n",
    "            data, label = read_tfrecord(path)  # Read each TFRecord file\n",
    "        except:\n",
    "            unread_paths.append(path)\n",
    "            continue\n",
    "        all_data.append(data)\n",
    "        all_labels.append(label)\n",
    "        count = count+1\n",
    "        if count == 512:\n",
    "            # Concatenate all data and labels\n",
    "            concatenated_data = torch.cat(all_data, dim=0)\n",
    "            concatenated_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "            torch.save({'data': concatenated_data, 'labels': concatenated_labels}, f'records/{type}/data_batch_{batch_number}.pth')\n",
    "\n",
    "            # Reset\n",
    "            all_data = []\n",
    "            all_labels = []\n",
    "            count = 0\n",
    "            \n",
    "            batch_number = batch_number + 1\n",
    "\n",
    "    # Concatenate all data and labels\n",
    "    concatenated_data = torch.cat(all_data, dim=0)\n",
    "    concatenated_labels = torch.cat(all_labels, dim=0)\n",
    "    # Save\n",
    "    torch.save({'data': concatenated_data, 'labels': concatenated_labels}, f'records/{type}/data_batch_{batch_number}.pth')\n",
    "    return unread_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6394/6394 [24:16<00:00,  4.39it/s]  \n"
     ]
    }
   ],
   "source": [
    "directory = './raw_records/train'\n",
    "file_paths = get_file_paths_shuffled(directory)\n",
    "unread_files = concatenate_and_save_tfrecords(file_paths, 'train')\n",
    "# for file in unread_files:\n",
    "#     shutil.move(file, './raw_records/train_unread/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2131/2131 [07:36<00:00,  4.66it/s]\n"
     ]
    }
   ],
   "source": [
    "directory = './raw_records/test'\n",
    "file_paths = get_file_paths_shuffled(directory)\n",
    "unread_files = concatenate_and_save_tfrecords(file_paths,'test')\n",
    "# for file in unread_files:\n",
    "#     shutil.move(file, './raw_records/test_unread/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2186/2186 [07:50<00:00,  4.64it/s]\n"
     ]
    }
   ],
   "source": [
    "directory = './raw_records/val'\n",
    "file_paths = get_file_paths_shuffled(directory)\n",
    "unread_files = concatenate_and_save_tfrecords(file_paths,'val')\n",
    "# for file in unread_files:\n",
    "#     shutil.move(file, './raw_records/val_unread/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "def concatenate_and_save_tfrecords_as_hdf5(tfrecord_paths, type):\n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "    unread_paths = []\n",
    "    count = 0\n",
    "    batch_number = 1\n",
    "    for path in tqdm.tqdm(tfrecord_paths):\n",
    "        try:\n",
    "            data, label = read_tfrecord(path)  # Make sure read_tfrecord function is defined\n",
    "            data = data.squeeze()\n",
    "        except:\n",
    "            unread_paths.append(path)\n",
    "            continue\n",
    "        all_data.append(data.numpy())  # Convert data to NumPy array\n",
    "        all_labels.append(float(label))  # Convert label to float and append\n",
    "        count += 1\n",
    "        if count == 512:\n",
    "            with h5py.File(f'records/{type}/data_batch_{batch_number}.h5', 'w') as h5f:\n",
    "                h5f.create_dataset('data', data=np.stack(all_data), compression='gzip')\n",
    "                h5f.create_dataset('labels', data=np.array(all_labels, dtype=np.float32), compression='gzip')\n",
    "            all_data = []\n",
    "            all_labels = []\n",
    "            count = 0\n",
    "            batch_number += 1\n",
    "\n",
    "    if all_data:  # Save any remaining data\n",
    "        with h5py.File(f'records/{type}/data_batch_{batch_number}.h5', 'w') as h5f:\n",
    "            h5f.create_dataset('data', data=np.stack(all_data), compression='gzip')\n",
    "            h5f.create_dataset('labels', data=np.array(all_labels, dtype=np.float32), compression='gzip')\n",
    "    return unread_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 602/6394 [03:31<24:45,  3.90it/s]   "
     ]
    }
   ],
   "source": [
    "directory = './raw_records/train'\n",
    "file_paths = get_file_paths_shuffled(directory)\n",
    "file_paths\n",
    "unread_files = concatenate_and_save_tfrecords_as_hdf5(file_paths, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2186/2186 [12:56<00:00,  2.82it/s]  \n"
     ]
    }
   ],
   "source": [
    "directory = './raw_records/val'\n",
    "file_paths = get_file_paths_shuffled(directory)\n",
    "unread_files = concatenate_and_save_tfrecords_as_hdf5(file_paths,'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2131 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2131/2131 [12:01<00:00,  2.96it/s]  \n"
     ]
    }
   ],
   "source": [
    "directory = './raw_records/test'\n",
    "file_paths = get_file_paths_shuffled(directory)\n",
    "unread_files = concatenate_and_save_tfrecords_as_hdf5(file_paths,'test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
